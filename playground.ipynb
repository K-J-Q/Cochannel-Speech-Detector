{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "config['augmentations']['pad_trunc_noise_multiplier']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianquan/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] saved_model/CNNtest_epoch0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jianquan/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ml.machineLearning as ml\n",
    "\n",
    "ml.selectModel()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchaudio\n",
    "import IPython.display as ipd\n",
    "\n",
    "trimmed_audio, b = torchaudio.load(\n",
    "    'E:/asdas.wav')\n",
    "\n",
    "trimmed_audio = trimmed_audio[0][0:1000000]\n",
    "print(trimmed_audio.shape)\n",
    "trimmed_audio = torchaudio.functional.vad(trimmed_audio, b)\n",
    "print(trimmed_audio.shape)\n",
    "ipd.Audio(trimmed_audio, rate=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import random\n",
    "import utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_paths, device):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sample_rate = torchaudio.load(self.audio_paths[idx])\n",
    "        waveform = waveform[0].to(self.device)\n",
    "        pad_len = 240000 - len(waveform)\n",
    "        zero_pad = torch.zeros(pad_len, device=self.device)\n",
    "        waveform = torch.concat((waveform, zero_pad),0)\n",
    "        return waveform\n",
    "\n",
    "audioPaths = list(Path('E:/Singapore Speech Corpus/Part 2.1').glob('**/*.wav'))\n",
    "dataset = AudioDataset(audioPaths, torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "training_loader = torch.utils.data.DataLoader(dataset, batch_size=1024, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "\n",
    "class MyPipeline(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_freq=16000,\n",
    "        resample_freq=8000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.resample = torchaudio.transforms.Resample(orig_freq=input_freq, new_freq=resample_freq)\n",
    "\n",
    "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        # Resample the input\n",
    "        processed = self.resample(waveform)\n",
    "        processed = torch.split(processed, 4*8000, dim=1)\n",
    "        return processed\n",
    "\n",
    "\n",
    "pipeline = MyPipeline()\n",
    "\n",
    "pipeline.to(device=torch.device(\"cuda\"))\n",
    "\n",
    "for batch in tqdm(training_loader):\n",
    "    pipeline(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(16000).reshape(16,1000)\n",
    "torch.split(a, 100, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "# from torchaudio.io import StreamReader\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 2\n",
    "RATE = 8000\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"* recording\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    print(data)\n",
    "    frames.append(data)\n",
    "\n",
    "print(\"* done recording\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loader.AudioDataset as Augmentation\n",
    "import torch\n",
    "from loader.AudioDataset import createDataset\n",
    "from torch.utils.data import Dataset\n",
    "import audiomentations\n",
    "import loader.utils as utils\n",
    "\n",
    "env_paths = utils.getAudioPaths('E:/Processed Audio/ENV')\n",
    "\n",
    "speech_paths = utils.getAudioPaths('E:/Processed Audio/SPEECH')\n",
    "\n",
    "audio_train_dataset = createDataset(env_paths, speech_paths, transformParams = utils.getTransforms(True))\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    audio_train_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "batch = next(iter(test_dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loader.utils as utils\n",
    "\n",
    "train, val = utils.getAudioPaths('E:/Processed Audio', percent=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "873 873\n",
      "98 98\n"
     ]
    }
   ],
   "source": [
    "print(len(train[0]), len(train[1]))\n",
    "print(len(val[0]), len(val[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2] [3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "import torchaudio, torch\n",
    "import loader.AudioDataset as AudioDataset\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = [0,1,2,3,4,5]\n",
    "print(a[:3],a[3:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
